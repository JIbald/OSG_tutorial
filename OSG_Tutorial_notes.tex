\documentclass[a4paper,12pt]{book}
%\usepackage[a4paper, inner=1.7cm, outer=2.7cm, top=2cm, bottom=2cm, bindingoffset=1.2cm]{geometry}

\usepackage{textcomp}
\usepackage[english]{babel}
\usepackage{blindtext}
%\usepackage[scaled=.92]{helvet}
\usepackage{microtype} %improve justification acorr entire document
\usepackage{graphicx} %includes pictures of any type
\usepackage{wrapfig} %wrap text around your pictures
\usepackage{enumitem} %lis looks nicer, condense spacing before items in list
%\usepackage{fancyhdr} %fancy headers etc.
\usepackage{amsmath} %mathformulas
\usepackage{index} %automatically generated indexes
%\usepackage{paralist} %compacts list as well
%\usepackage[onehalfspacing]{setspace} %also singlespacing, doublespacing (documentwide)

\usepackage{listings}
\usepackage{xcolor}
\lstset { %
    language=C++,
    backgroundcolor=\color{black!5}, % set backgroundcolor
    basicstyle=\footnotesize,% basic font setting
}

%on Linux / MacOS:
\usepackage[utf8]{inputenc}

\newcommand{\NTT}{New Think Tank}
\newcommand{\NTTB}{\textbf{New Think Tank}}
\newcommand{\typew}[1]{\texttt{#1}}

\makeindex

\begin{document}
\title{\Large{\textbf{Projects}}}
\author{Johannes Ibald}
\date{November 07, 2020}

\frontmatter
\pagenumbering{Roman} %roman numeral page numbering
\setcounter{page}{2} %start pagenumbering on specific page

\maketitle
\let\cleardoublepage\clearpage
\tableofcontents %lists all chapters and section headings


%\fancyhf{}
%\renewcommand{\headrulewidth}{2pt}
%\renewcommand{\footrulewidth}{1pt}
%\fancyhead[LE]{\leftmark}
%\fancyhead[RO]{\nouppercase{\rightmark}}
%\fancyfoot[LE,RO]{\thepage}

%\pagebreak %also stretches text. If not wanted, do \newpage
%\newpage
\mainmatter
\pagenumbering{arabic}
\chapter{Resource Latex}

\blindmathtrue
\blindtext[5]

\section{A Section}
\enlargethispage{\baselineskip} %might squeeze left over text to page before 
\blindtext[2]
\blinditemize
\blindenumerate
\blinddescription


\section{Cool Pictures}

\begin{figure}[ht]  %h: here, t: top of page, b: bottom, p: in center of page
\centering
%\includegraphics[width=8cm]{pictures/Itsuki_def_2.png}
\caption{Itsuki heard some shocking information}\label{fig:Ituski_shocked}
\end{figure}
\blindtext[2]
\newpage


\begin{enumerate}[label=\Roman*, font=\bfseries]
	
	\item Material	
	\begin{itemize}
		\item Fichte ?
	\end{itemize}		
	
	\item Deskplate
	\begin{itemize}
		\item amount: 1
		\item measurements: 2500 x 1250 x 27 [mm]
	\end{itemize}

	\item Reinforcement
	\begin{itemize}
		\item amount: 1
		\item Length: 2500 mm
		\item Width: $\leq$ 90 mm
		\item Height: 160 $>$ H $>$ 140 [mm]
		\item width (W) is determined by two Ikea Alex but proper legs might give more flexibility later on	
	\end{itemize}
\end{enumerate}

\chapter{}
\chapter{•}
\chapter{•}
\chapter{•}
\chapter{Creating Realistic Rendering Effects}

\section{Understanding graphics shaders}
\begin{enumerate}

\item OpenGL shading language (GLSL) provides the ability to develop graphics shaders

\textrightarrow blocks of graphic software instructions to calculate more relistic rendering effects, rather than fixed function states.

\item steps to desing shaders and applying them to a sg
\begin{itemize}
\item write your own shaders ("like C programs"). They are treated as a set of strings passed to the hardware so create them on the fly or read them as text files.


\item specify no more than a vertex shader, a geometry shader and a fragment shader to be processed in the OpenGL pipeline. Each stage has only one main() function.

\item will totally replace fixed functionalities such as fog, lighting and texture mapping, which have to be re-implemented in your shader source code.

\item Shaders require OpenGL API to compile and execute them.
\item Vertex shader scan apply transformations to each vertex
\item Fragment shaders calculate the color of infividual pixels coming from the rasterizer;
\item Geometry shaders re-generate geometries from existing vertices and primitive data

\end{itemize}

\end{enumerate}

\subsection{osg::Shader}
\begin{itemize}
\item define shader object containing source code strings.
\item setShaderSource() specifies the src code text from a std::string variable
\item loadShaderSourceFromFile() reads a source file from drive.

\item construct shader object from existing string like this:

\begin{lstlisting}
osg::ref_ptr<osg::Shader> vertShader = 
	new osg::Shader(osg::Shader::VERTEX, vertText);
\end{lstlisting}

\item input param OSG::SHader::VERTEX represents the vertex shader. Use GEOMETRY or FRAGMENT enums instead to specify geometry- or fragment shader.

\begin{lstlisting}
osg::ref_ptr<osg::Shader> fragShader = 
	new osg::Shader( osg::Shader::FRAGMENT, fragText );
	
osg::ref_ptr<osg::Shader> geomShader = 
	new osg:Shader( osg::Shader::GEOMETRY );
	
geomShader -> loadShaderSourceFromFile( "source.geom" );

\end{lstlisting}
\textrightarrow source.geom contains geometry shader.

\item osgDB::readShaderFile() may be even better\\
\textrightarrow automatically checks shader types (via extensions: .vert, .frag, .geom)\\
\textrightarrow returns osg::Shader instance of correct type and data:\\
\begin{lstlisting}
osg::Shader* fragShader =
	osgDB::readShaderFile("source.frag");

\end{lstlisting}
\textrightarrow shaders are set and ready to be use\\
\textrightarrow use osg::Program calss and addShader() method to include include shaders and set GLSL rendering attribute and modes to a state set.\\

\item most other fixed-function states willbecome incalid after the shaders make effects, including lights, materials, fog, texture mapping, texture coordinate generation and texture environment.\\

\item following code adds all above shaders to an osg::Program objectand attaches it to the state set of existing node:

\begin{lstlisting}
osg::ref)ptr<osg::Program> program =
	new osg::Program;
program -> addShader( vertShader.get() );
program -> addShader( fragShader.get() );
program -> addShader( geomShader.get() );
node -> getOrCreateStateSet() -> setAttributeAndModes(
	program.get() );
\end{lstlisting}

\end{itemize}

\section{Using uniforms}

\begin{itemize}
\item three types of inputs and outputs in a typical shader:\\
\textrightarrow uniforms\\
\textrightarrow vertex attributes\\
\textrightarrow varyings\\

\item Uniforms and Vertex Attributes are read-only during the sahder's exevution, but can be set by host OpenGL or OSG apps.

\textrightarrow They are actually global GLSL variables used for interactions between shaders and user applications.\\

\item Varyings are used for passing data from one shader to the next one\\
\textrightarrow tehy are invisible to external programs\\
\item OSG uses osg::Uniform class
\end{itemize}

\subsection{osg::Uniform class}
\begin{itemize}
\item used to define a SLSL uniform cariable
\item constructor has a name and initial value param, which should match the definition in the shader souce code, e.g:

\begin{lstlisting}
float length = 1.0f;
osg::ref_ptr<osg::Uniform> uniform = 
	new osg::Uniform( "length", length );
\end{lstlisting}

\item add uniform object to state set, which has attached osg::Program object via addUniform():\\

\begin{lstlisting}
stateset -> addUniform( uniform.get() );
\end{lstlisting}
There should be a variable defined in one of the shader sources:\\
\begin{lstlisting}
uniform float length;
\end{lstlisting}

Otherwise, uniform cariable will not be availabel in either OSG programs or shaders.\\

\item Uniforms can be any basic type, or any aggregation of types, such as Boolean, float, integer, 2D/3D/4D vector, matrix and various texture samplers.\\
\item osg::Uniform class accepts all basic types with constructor and set() method.\\
\textrightarrow additionally, osg::Matrix2 and osg::Matrix3\\

\item to bind texture sampler ( used in shaders to represent a particular texture) you specify the texture mapping unit by using an unsigned int:\\
\begin{lstlisting}
osg::ref_ptr<osg::Uniform> uniform = 
	new osg::Uniform( "texture", 0 );
\end{lstlisting}

\item there must already be an osg::Texture object at unit 0, as well as a samplet uniform in the shader source:\\
\begin{lstlisting}
uniform sampler2D texture;
\end{lstlisting}
\textrightarrow assume that it's a 2D texture that will be used to change the shader's executing behavior.

\end{itemize}
\subsection{Time for Action page 154}
\subsection{What just happened?}
basic alorithm for caroon shading:
\begin{itemize}
\item if there's a normal that is close to the light direction, the brightest tone\\ \textrightarrow color1 is used.
\item as the angle between light direction and surface normal is increasing\\
 \textrightarrow darker tones will be used (color2, color3, color4)\\
\textrightarrow provides an intensity value for selecting tones.\\
\textrightarrow all four tones are declares as 4D vectors in FRAGMENT SHADER and passed to osg::Uniform objects as osg::Vec4 variables in the user app.
\end{itemize}

\section{Working with the geometry shader}
\begin{itemize}
\item geometry shader is included into the OpenGL 3.2 core\\
\textrightarrow in lower versions it is udes as an extension (\verb|GL_EXT_ geometry_shader4| ) which should be declared in the shader sourve code.
\item geometry shader has new sdjacency primitives\\
\textrightarrow can be used as arguments of osg::PrimitiveSet derived classes.\\
\textrightarrow also requires setting up params in order to maipulate the shader operations:

\begin{enumerate}
\item \verb|GL_GEOMETRY_VERTICES_OUT_EXT|: nums of vertices that the shader will emit
\item \verb|GL_GEOMETRY_INPUT_TYPE_EXT|: the primitive type to be sent to the shader
\item \verb|GL_GEOMETRY_OUTPUT_TYPE_EXT|: primitive type to be emitted from the shader
\end{enumerate}
\textrightarrow osg::Program class's setPatameter() sets values for these params\\
\textrightarrow 100 vertices wil be emitted from the shader to the primitive assembly processor in the rendering pipeline:
\begin{lstlisting}
program -> setParameter( GL_GEOMETRY_VERTICES_OUT_EXT, 100 );
\end{lstlisting}
\end{itemize}

\subsection{Time for action - Generating a Bezier curve}
P158
\subsection{What just happened?}
\begin{itemize}
\item geometry shader defines a new primitive type \verb|GL_LINE_STRIP_ADJACENCY_EXT| which means a line strip with adjacency\\
\textrightarrow first and last vertices provide adjacency information bur aren't visible as line segments.\\
\textrightarrow thus we can use these two extra vertives as the endpoints of a Bezier curve and the others as control points\\
\textrightarrow that is actually what we read from the GLSL variable \verb|gl_Position[0]| to \verb|gl_PositionIn[3]|.\\

\item Cubic Bezier curve equation:\\
%\begin{lstlisting}
$P(t) = (1-t)^3 *P0 + 3 * t * (1-t)^2 * (1-t)*P2 + t^3 *P3 \; with \;  0 \leq t \leq 1$
%\end{lstlisting}
\end{itemize}
See summary.
\chapter{Viewing the World}
Focus:
\begin{itemize}
\item understandig the coordinate system defined in OpenGL
\item alternating the view point and orientation, projection frustum, and final viewport
\item changing and controlling the rendering order if there exists more than one camera
\item how to create single and composite viewers
\item how to manage global dispay settings and generate easy-to-use stereo visualization effects
\item how to apply the rendered scene as a texture object - so called rendering to textures (RTT)
\end{itemize}

\subsection{From world to screen}
this subsection will be shorter, since a version of this is already in my personal notebook.

\subsubsection{modelmatrix}
used to describe the specific location of an object in the world.\\
\textrightarrow transforms object's local coord sys to world coord sys. Both coord. systems are right-handed.\\

\subsubsection{view matrix}
\textrightarrow transforms entire world into view space.
suppose we have a camera placed at a vertain position in the world; the inverse of the camera's transformation matrix is actually used as the view matrix.\\
In the right-handed view coord sy, OpenGL defines that the camera is always located at the origin (0, 0, 0), and facing towards the negative Z axis.\\
\textrightarrow Hence, we can represent the world on our camera's screen.\\

\subsubsection{Note:}
There is no separate model matrix or view matrix in Open GL.\\
\textrightarrow however, it defines a model-view matrix to transform from the object's local space to view space, which is a combination of both matrices.\\
\textrightarrow to transform vertex V in local space to Ve in view space, we have:\\
$Ve = V * modelViewMatrix$

\subsubsection{projection matrix}
we have to:
\begin{itemize}
\item determine how 3D objects are projected onto the screen (perspective or orthogonal)
\item calculate the frustum.\\
\\
\textrightarrow Projection matrix is used to specify the frustum in the world coordinate system with six clipping planes: left, right, bottom, top, near and far planes.\\
\\
\textrightarrow OpenGl function: gluPerspective(), determines a field of view with camera lens params.
\item resulting coord sys is called: Normalized Device Coordinate System\\
\textrightarrow it ranges from -1 to +1 in each of the axed.\\
\textrightarrow is changed to left-handed now.
\item as a final step:\\
project all result data onto viewport. (the window)\\
define the window rectangle in which the final image is mapped\\
As well as Z Value of the window coordinates.
\item Now the 3D scene is rendered to a rectangular area on your 2D screen. 

\end{itemize}
\subsubsection{MVPW matrix}

Finally, the screen coord $Vs$ can represent the local vertex $V$ in the 3D world by using the so called MVPW matrix:\\

$Vs = V * modelViewMatrix * projectionMatrix * windowMatrix$\\

The $Vs$ is still a vector that represents a 2D pixel location with a depth value.\\
By reversing this mapping process, we can get a line in the 3D space from a 2D screen point ($Xs, \; Ys$)\\
\textrightarrow that's because th 2D point can actually be treated as two points: one on the near clipping plane $(Zs \; =0)$ and the other on the far plane $(Zs=1)$.\\

The inverse of the MVPW matrix is used to obtain the result of the "unproject" work:\\
$V0=(Xs, \; Ys, \; 0) \; * \; invMVPW$\\
$V1 = (Xs, \; Ys, \; 1) \; * \; invMVPW$\\

\section{The Camera class}
\begin{itemize}
\item it's popular to use glTranslate() and glRotate()\\
\textrightarrow moves the scene
\item it's popular to use gluLookAt()\\
\textrightarrow moves the camera

\item though they are all replaceable by glMultMatrix()\\
\textrightarrow in fact, these functions do the same thing: calculate the model-view matrix for transforming data from world space to view space.
\item similarly, OSG had osg::Transform class\\
\textrightarrow adds or sets its own matrix to the current model-view matrix when placed in the sg 
\item BUT: we always intend to operate on model matrix by using the\\
\textrightarrow osg::MatrixTransform and osg::PositionAttitudeTransform subclasses\\
\textrightarrow we handle the view matrix with the osg::Camera subclass.
\item osg::Camera class is one of the most important classes in the core OSG libraries.\\
\textrightarrow can bes used as Group node
\item but it is far more than a common node\\
\textrightarrow main functionalities in four categories:
\begin{enumerate}
\item osg::Camera class handles the view matrix projection matrix and viewpoert\\
\textrightarrow affects all its chilfren and project them onto the screen\\
Related methods:
\begin{itemize}
\item public: setViewMatrix() and setViewMatrixAsLookAt() methods set the view matrix by using the osg::Matrix variable or classic eye/center/up variables.
\item public setProjectionMatrix() method accepts an osg::Matrix parameter in order to specify the projection matrix
\item other convenient methods:\\
\textrightarrow setProjectionMatrixAsFrustum()\\
\textrightarrow setProjectionMatrixAsOrtho()\\
\textrightarrow setProjectionMatrixAsOrtho2D()\\
\textrightarrow setProjectionMatrixAsPerspective\\
are used to set a perxpevtive or orthographic projection matrix with different frustum parameters.\\
they work just like the OpenGL projection functions (..., see page 165)
\item public setViewport() method defines a rectangular window area with an osg::Viewpoert object.
\end{itemize}
set view and projection matrix of a camera node, set its viewport to $(x,y) - (x+w, y+h)$:
\begin{lstlisting}
camera -> setViewMatrix( viewMatrix );
camera -> setProjectionMatrix( projectionMatrix );
camera -> setViewport( new osg::Viewport( x, y, w, h ) );
\end{lstlisting}
Obtain current view and projection matrices and viewpoert of the osg::Camera object by using the correspoinding get*() methods at any time, e.g.:
\begin{lstlisting}
osg::Matrix viewMatrix = camera -> getViewMatrix();
\end{lstlisting}
get position and orientation of view matrix:
\begin{lstlisting}
osg::Vec3 eye, venter, up;
camera -> getViewMatrixAsLookAt( eye, center, up);
\end{lstlisting}
\item osg::Camera encapsulates the OpenGl functions, such as glClear(), glClearColor(), and glClearDepth(), and clears the frame buffers and presets their values when redrawing the scene to the window in every frame.\\
Primary methods include:
\begin{itemize}
\item setClearMask() method, sets buffer to be cleared.\\default:
\begin{lstlisting}
GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT
\end{lstlisting}

\item setClearColor() method sets the clear color in RGBA format, by using an osg::Vec4 variable.

\item similarly there's setClearDepth(), setClelarStencil(), setClearAccum() (and their get*() methods)
\end{itemize}

\item third category includes the management of OpenGL graphgics context associated with this camera.\\
\textrightarrow Chapter 9 Interacting with Outside Elements

\item Finally, a camera can attach a texture object to internal buffer coponents (color buffer, depth buffer, and so on) and directly render the sub-scene graph into this texture.\\
\textrightarrow the resultant texture can then be mapped to surfaces of other scenes. This techique is named render-to-textures or texture baking \textrightarrow later this chapter.

\end{enumerate}

\end{itemize}

\subsection{Rendering order of cameras}
\begin{itemize}
\item at least one main camera node in any sg.\\
\textrightarrow created and managed by the osg::ViewerViewer class\\
\textrightarrow read it with getCamera() method.
\item It automatically adds the root node as its child node before starting the simulation.\\
\textrightarrow by default all other cameras (directly and indirectly added to root node) will share the graphics context associated with the main camera, will share the graphics context associated with the main camera + draw their their own sub-scenes successively onto the same rendering window.

\item osg::Camera class provides setRenderOrder() method to precisely control the rendering order of cameras.\\
\textrightarrow It has an order enum and an optional order num param.\\
\textrightarrow first enum is either \verb|PRE_RENDER| or \verb|POST_RENDER| (indicates general rendering order\\
\textrightarrow second is an interger num for sorting cameras of the same type in ascending order. (default = 0)
\item this will force OSG to render camer1 first, then camera2 (larger int num ), then camera3:
\begin{lstlisting}
camera1 -> setRenderOrder( osg::Camera::PRE_RENDER );
camera2 -> setRenderOrder( osg::Camera::PRE_RENDER, 5 );
camera3 -> setRenderOrder( osg::Camera::POST_RENDER );
\end{lstlisting}

\end{itemize}

If a camera is rendered first (\verb|PRE_RENDER|)it's rendering result in the buffers will be cleared and covered by the next camera, and the viewer may not be able to see its sub-scene. This is especaially useful in the case of the render-to-textures process, because we want the sub-scene to be hidden from the screen, and update the attached texture objects before starting the main scene.\\

In addition, if a camera is rendered afterwards (\verb|POST_RENDER|), it may erase the current color and depth values in the buffers.\\
\textrightarrow avoid this by calling setCLearMask() with fewer buffer maks.(HUD head up display)

\subsection{Time for action creating an HUD camera}
P168

\subsection{WTF just happened?}
an additional camera contains the glider model that is to be rendered as its sub-scene-graph on top of the rendering result(color buffer and depth buffer) of the main camera.\\
The additional camera's goal is to implement a HUD scene that overlays the main scene. It clears the depth buffer to ensure that all pixel data drawn by this camera can pass the depth test. However, the color buffer is not cleared, keeping the uncovered pixel data of the main scene on the screen. That is why we set it up like this;
\begin{lstlisting}
camera -> setClearMask( GL_DEPTH_BUFFER_BIT ); // no color buffer bit
\end{lstlisting}

\subsection{using a single viewer}

OSG supports the single viewer class osgViewer::Viewer for holding a view on a single scene.\\
setSceneData() method\\
\textrightarrow manages the scene graph's root node\\


run()\\
\textrightarrow starts the simulation loop (scene is rendered per frame)
\textrightarrow the frame buffer is updated continuously by the result of every rendering cycle (-> frame)\\

the viewer also contains an osg::Camera object as the main camera.\\
View Matrix of the camera is controlled by the viewer's internal osgGA::CameraManipulator object.\\

User input events are received and handled by the viewer as well..\\
\textrightarrow this works via a list of osgGA::GUIEventHandler handlers.\\

The viewer can even be set up in full screen mode, in a window and onta a spherical display.

\subsection{Digging into the simulation loop}
The simularion loop defined by the run() method always has three types of tasks to perform:
\begin{enumerate}
\item specify the main camera's manipulator
\item set up associated graphics contexts
\item render frames in cycles
\end{enumerate}

The manipulator can read keyboard and mouse events and accordingly adjust the main camera's view matrix to navigate the scene graph.\\
\textrightarrow set by using setCameraManipulator() method\\
\textrightarrow param: osgGA::Cameramanipulator subclass\\
e.g.:
\begin{lstlisting}
viewer.setCameraManipulator( new osgGA::TrackballManipulator );
\end{lstlisting}

\textrightarrow adds trackball(arc ball) manip to viewer object, (free motion behavious)\\
\textrightarrow because camera manipulator is kept a smart pointer in the viewer, we can assign a new manip by using the setVameraMAnipulator() method any time.

see page 170 for table with maipulators\\
\textrightarrow beware , to declare and use a manip you should add the osgGA library as a dependence of your project\\
\textrightarrow CMake scripts

the graphics contexts of a viewer, as well as possible threads and resourfes, are all initialized ni the realize(0 method\\
\textrightarrow automatically called before the first frame is rendered\\
Now the viewer enters the loop:\\
\textrightarrow each time frame() method is used to render a frame. It checks if the rendering process should stop and exit with the don() method. The process can be described with just a few lines of code:
\begin{lstlisting}
while( !viewer.done() )
{
	viewer.frame();
}
\end{lstlisting}

\textrightarrow default rendering scheme used in the viewer class.\\
Frame rate is synched to the monitor's refresh rate to avoid wasting system energy, if the vsync option of the gpu is on.\\
OSG supports another on-demand rendering scheme:
\begin{lstlisting}
viewer.setRunFrameShceme( osgViewer::Viewer::ON_DEMAND );
\end{lstlisting}
noe the frame() method will only be executed when there are scene graph mods, updating processes, or user input events, until the scheme is changed back to the default value of CONTINUOUS.\\

As an addition, the osgViewr::Viewer class also contains a setRunMaxFrameRate() method which uses a frame rate number as the param.\\
\textrightarrow can set a max frame rate\\
\textrightarrow controls viewer running to force rendering frames without lots of consumption.

\subsection{Time for action - custom simulation loop}

run() was used many times.\\
\textrightarrow performed update, cull ad draw traversals each frame.\\
\textrightarrow see P172.
\subsection{What on earth just happended?}

this was the concept of pre- and post-frame events and assume they are executed before and after frame() method.\\
\textrightarrow inaccurate.\\

multiple threads are used to manage user updating, culling and drawing of different cameras.\\
\textrightarrow especially with multiple screens, processors, and gpu.\\
frame() method only starts a new updating/culling/drawing traversal work, but does not take care of thread synchronization. In this case, the code before and after frame() will be considered unstable and unsafe, bevause they may conflict with other process threads when reading or writing the scene graph.\\
\textrightarrow so the approach described here is not recommended for future development.\\
\textrightarrow "correct" methods in next chapter.\\

When will the viewer.done() return true?\\
Of course, you can set the done flag via setDone() meghod of viewe
OSG system will check if all present graphics contexts (for example, the rendering window)have been closed, or if the Esc key is pressed which will also change the done flag.\\
setkeyEvenetSetsDone() method can even set which key is going to carry out the duty instead of the default Esc ( osr set this to 0 to turn off the feature).\\

\subsection{Using a composite viewer}
osgViewr::VIewer class manages only one single view on one scene graph.\\
osgViewer::CompositeViewer class supports multiple views and multiple scenes.\\
This has the same methods such as run(), frame() and done(0 to manage the rendering process, but also supports adding and removing independent scene views by using the addView() and removeView() meghods, and obtaining a view object at a specific index by using the getView() method. The view object here is defined by the osgViewer::View class.\\
osgViewer::View class is the super class of osgViewer::Viewer\\
\textrightarrow it accepts setting a root node as the scene data, and adding a camera manipulator and event handlers to make use of user events as well.\\
Ther main difference between osg::Viewer::View and osgViewer::Viewer is that the former cannot be used as a single viewer directly - that is, it doesn't have run() or frame() methods.\\

To add a created view object to the composite viewer:
\begin{lstlisting}
osgViewer::CompositeViewer multiviewer;
multiviewer.addView( view );
\end{lstlisting}

\subsection{Time for action - rendering more scenes at one time}
Multi-viewers are practical in representing comlex scenes, for instance, to render a wide area with a main view and an eagle eye view, or to display the front, side, top, and persepective views of the same scene. Here we eill create three separate windows, containintg three different models, each of which can be independently manipulated.
\subsection{What the fuck just happened?}
it's possible to create three osg::Camera nodes, add different sub-scenes to them, and attach them to different graphics contexts (rendering window) in order to achieve the same result as the previous image. \\
\textrightarrow every osgViewer::View object has an osg::Camera node that can be used to manage its subscene and associated window.\\
\textrightarrow it actually works like a container.\\

However, the osgViewer::View class handles manipulator and user events, too.\\
\textrightarrow in a composite viewer, each osgViewer::View object holds its own manipulator and event handlers (this will be siscussed in Chapter 9, Interacting with Outside Elements).\\
However, a set of cameras can hardly interact with user inputs separately. That is why we choose to use a composite viewer and a few view objects to represent multiple scenes in some cases.

\subsection{Changing global display settings}
OSG manages a set of global display settings that are required by cameras, viewers, and other scene elements. It uses the singleton pattern to declare a unique instance of the container of all of these settings, by using the osg::DisplaySettings class. We can thus obtain the display settings instance at any time in our apps:
\begin{lstlisting}
osg::DisplaySettings* ds = osg::DisplaySettings::instance();
\end{lstlisting}
The osg::DisplaySettingsa instance sets up properties requested by all newly created rendering devices,  ainly OpenGL graphics contexts of rendering windows. Its chracteristics include:
\begin{enumerate}
\item setDoubleBuffer() method: set double or single buffering. Default is on.
\item setDepthBuffer() method: whether to use depth buffer or not. Default is on.
\item setMinimumNumAlphaBits() (and others): set bits for an OpenGL alpha buffer, a stencil buffer, accumulation buffer. Defaults are all 0.
\item setNumMultiSamples(): set using multisampling buffers and number of samples. default is 0.
\item enable stereo rendering and configure stereo mode and eye mapping parameters
\end{enumerate}

\textrightarrow some of these characteristics can be separately set for different graphics contexts by sing a specific traits structure. For now we use global display settings.

\subsection{Time for action - enabling global multisampling}

P180

\subsection{wth just happened?}
multisampling allows apps to create a frame buffer with a given number of samples per pixel.\\
\textrightarrow contains color depth, stencil info.\\
\textrightarrow more video memory is required but a better rendering result will be produced.\\
OSG has an internal graphics context manager osg::GraphicsContext:\\
\textrightarrow it's subclass osg::GraphicsWindowWin32 ( look up Linux version ) manages the config and creation of rendering windows under Windows.\\
It will apply these two attributes to the encapsulated wglChoosePixelFormatARB() function and enable multisampling of the entire scene.\\
osg::DisplaySettings actually works like a default value set of various display attributes. If there is no separate setting for a specific object, the default one will take effect; Otherwise the osg::DisplaySettings instance will not be put to use.\\
We are going to talk about the separate settings for creating graphics context and the osg::GraphicsContext class in Chapter 9

\subsection{Stereo visualization}
We have already experienced the charm of stereoscopic 3D films and photographs.\\
\textrightarrow James Cameron's Avatar\\
Anaglyph image is the earliest and most popular method of presenting stereo visualiyation. \\
others: NVIDIA's quad-buffering, horizontal or vertical split, horizontal or vertical interlace, ...\\
Fortunately, OSG supports most of these common stereo techniques, and can immediately relize one of them in the viewer with just a few commands:\\

\begin{lstlisting}
osg::DisplaySettings::instance() -> setStereoMode( mode );
osg::DisplaySettings::instance() -> setStereo( true );
\end{lstlisting}

The method setSteroMode(0 selects a stereo mode from a set of enumerations, and the setStereo() meghod enables or disables it. Available stereo modes in OSG are: \verb|ANAGLYPHIC, QUAD_BUFFER (NVIDIA ) , HORIZONTAL_SPLIT, VERTICAL_SPLIT, HORIZONTAL_INTERLACE, VERTICAL_INTERLACE, and CHEKERBOARD| (DLP projector).\\
You may also use \verb|LEFT_EYE or RIGHT_EYE| to indicate that the screen is used for left-eye or right-eye views.\\
for more stereo params, such as the eye separation, have a look at the API documentation.

\subsection{Time for action - rendering naglyph stereo scenes}
P183

\subsection{wtf just happened?}
in the \verb|ANAGLYPHIC| mode, the final rendering result is always made yp of two color layers, with a small offset to produce a depth effect. Each eye of the glasses will see aslightly different picture, and their composition produces a sterograph image, which will be fused by our brain into a three dimensional scene.\\
OSG suports the anaglyphic stereo mode with a two-pass rendering scheme. The first pass renders the left eye image with a red channe color mast, and the second pass renders the right eye image with a cyan channel. the color mask is defined by the rendering attribute osg::ColorMask. It can be easily applied to state sets of nodes and drawables by using:

\begin{lstlisting}
osg::ref_ptr <osg::ColorMask> colorMask = new osg::ColorMask;
colorMask -> setmask( true, true, true, true );
stateset -. setAttribute( colormask.get() );
\end{lstlisting}

stero mode often causes the scene graph to be rendered multiple times, which sloes down the frame rate as a side effect.

\subsection{Rendering to textures}
the render to textures technique allows developers to create textures based on a sub-scene's appearance in the rendered scene.
These textures are then "baked"into objects of coming scg via texture mapping. They can be used to create nice specital effects on the fly, or can be stored for subsequent deferred shading, multi-pass rendering, and other advanced rendering algorithms.\\
To implement texture baking dynamically, there are generally three steps to follow:
\begin{enumerate}
\item Create the texture for rendering.
\item Render the scene to the texture.
\item Use the texture as you want.
\end{enumerate}

We have to create an empty texture object before putting it into use. OSG can create an empty osg::Texture object by specifying its size. The setTextureSIze() method defines the width and height of a 2D texture, and an additional depth parameter of a 3D texture.\\

The key to rendering a scene graph to the newly created texture is the attach() method of the osg::Camera class. This accepts the texture object as an argument, as well as a buffer component parameter, which indicates which part of the frame buffer will be rendered to the texture. For example, to attach the color buffer of a camera's sub-scene to the texture, we use:
\begin{lstlisting}
camera -> attach ( osg::Camera::COLOR_BUFFER, texture.get() );
\end{lstlisting}

Other usable buffer components include the \verb|DEPTH_BUFFER|, \verb|STENCIL_BUFFER| and \verb|COLOR_BUFFER0| to \verb|COLOR_BUFFER15|(multiple render target outputs, depending on the gpu).\\
Continue setting suitable view and projection matrices of this camera, and a viewpoer to meet the texture size, and set the texture as an attribute of nodes or draeables. The texture will be updated with the camera's rendering result in every frame, dynamically carying with the alteration of the view matrix and the projection matrix.\\
Be aware that the main camera of a viewer is not suitable for attaching a texture. Otherwise there will be no ouputs to the actual window, which will make the screen pitch-dark. Of course, you ay ignore this if you are doing off-screen rendering and on't care of any visual effects.

\subsection{Frame buffer, pixel buffer, and FBO}
A concern is how to get the rendered frame bufdfer image into the texture object. A direct approach is to use the glReadpixels() method to return pixel data from the fram buffer, and apply the result to a glTexImage*() method.\\
\textrightarrow easy to conceptualize and use, but will always copy data to the texture object, which is extremely slow.\\

The glCopyTexSubImage() method would be better in terms of improving the efficiency. However, we can still optimize the process. Rendering the scene directly to a target other than the frame buffer is a good idea. There are mainly two solutions for this:
\begin{enumerate}
\item the pixel buffer (pbuffer for short) extension can create an invisible rendering buffer with a pixel format descriptor, which is equivalent to a window. It should be destroyed after being used, as is done for the rendering window.
\item The frame buffer object (FBO for shor), which is sometimes better than pixel buffer in saving the storage space, can add application-created fram buffers and redirect the rendering output to it. It can either output to a texture object or a renderbuffer object, which is imply a data storage object.
\end{enumerate}

OSG supports making use of different render target implementations: directly copying from the frame buffer pixel buffer, or FBO. It uses the method setRenderTargetImplementation() of the osg::Camera class to selsect a solution from them, for example:

\begin{lstlisting}
camera -> setRenderTargetImplementation( osg::Camera::FRAME_BUFFER );
\end{lstlisting}

This indicates that the rendering result of Camera will be rendered to the attached texture by using the glCopyTexSubImage() method internally. In fact, this is the default setting of all camera nodes.\\
Other major implementations include \verb|PIXEL_BUFFER| and \verb|FRAME_BUFFER_OBJECT|

\subsection{TFA - drawing aircrafts on a loaded terrain}
In this section , we are going to integrate what we learned before to create a slightly complex example, which identifies all texture objects in a scene graph by using the osg::NodeVisitor utility, replaces them with a newly created shared texture, and binds the new texture to a render-to-textures camera. the texture is expected to represent more than a static image, so a customized simulation loop will be used to animate the sub-scene graph before calling the frame() method.







\end{document}